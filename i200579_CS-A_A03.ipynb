{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2df1c360",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e183858",
   "metadata": {},
   "source": [
    "#### Importing File & PreProcessing (Normalization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a3e348b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_path):\n",
    "    # Loading the dataset into a dataframe\n",
    "    data = pd.read_csv(file_path)\n",
    "    \n",
    "    #Removing Timestamp column\n",
    "    features = data.drop(['timestamp_(min)'], axis=1, errors='ignore') \n",
    "\n",
    "    # Normalizing features\n",
    "    scaler = StandardScaler()\n",
    "    scaled_features = scaler.fit_transform(features)\n",
    "    tensor_data = torch.tensor(scaled_features, dtype=torch.float32)\n",
    "    return tensor_data\n",
    "\n",
    "# Loading and preprocessing data\n",
    "file_path = 'train.csv'\n",
    "data_tensor = load_data(file_path)\n",
    "\n",
    "# Creating a DataLoader\n",
    "train_loader = DataLoader(data_tensor, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac6e6fb2",
   "metadata": {},
   "source": [
    "#### Creating Binary Noise Mask Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62f94f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to create Noise Mask Matrix\n",
    "def generate_binary_noise_mask(num_samples, num_features, masked_proportion, mean_length):\n",
    "    # Calculating transition probabilities\n",
    "    pm_to_u = 1 / mean_length  \n",
    "    pm_to_m = 1 - pm_to_u  \n",
    "    pu_to_m = pm_to_u * (masked_proportion / (1 - masked_proportion))\n",
    "    pu_to_u = 1 - pu_to_m     \n",
    "\n",
    "    mask = np.zeros((num_samples, num_features), dtype=int)\n",
    "    \n",
    "    for feature in range(num_features):\n",
    "        state = np.random.choice([0, 1], p=[1 - masked_proportion, masked_proportion])\n",
    "        mask[0, feature] = state\n",
    "        \n",
    "        for i in range(1, num_samples):\n",
    "            if state == 1:\n",
    "                state = np.random.choice([0, 1], p=[pm_to_u, pm_to_m])\n",
    "            else:\n",
    "                state = np.random.choice([0, 1], p=[pu_to_m, pu_to_u])\n",
    "            mask[i, feature] = state\n",
    "\n",
    "    return torch.tensor(mask, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb63340",
   "metadata": {},
   "source": [
    "#### Creating Masked Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b8e2438",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to mask generated DataLoader\n",
    "def masked_dataloader(data_tensor, batch_size, masked_proportion, mean_length):\n",
    "    # Generating masks for all data\n",
    "    num_samples, num_features = data_tensor.size()\n",
    "    mask = generate_binary_noise_mask(num_samples, num_features, masked_proportion, mean_length)\n",
    "    masked_data = data_tensor * mask\n",
    "\n",
    "    # Create a DataLoader\n",
    "    dataset = TensorDataset(masked_data, data_tensor)  \n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "train_loader = masked_dataloader(data_tensor, batch_size=64, masked_proportion=0.2, mean_length=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd23c1fc",
   "metadata": {},
   "source": [
    "#### GAN Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f38457c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, feature_size, num_heads, num_layers, dropout=0.1):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        self.encoder_layers = nn.TransformerEncoderLayer(d_model=feature_size, nhead=num_heads, dropout=dropout)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(self.encoder_layers, num_layers=num_layers)\n",
    "\n",
    "    def forward(self, src):\n",
    "        return self.transformer_encoder(src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ac903fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPDecoder(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(MLPDecoder, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, output_dim)\n",
    "        self.fc2 = nn.Linear(output_dim, output_dim)\n",
    "        self.activation = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.activation(self.fc1(x))\n",
    "        return self.activation(self.fc2(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "733cee51",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, feature_size, num_heads, num_encoder_layers, output_dim):\n",
    "        super(Generator, self).__init__()\n",
    "        self.encoder = TransformerEncoder(feature_size, num_heads, num_encoder_layers)\n",
    "        self.decoder = MLPDecoder(feature_size, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        latent = self.encoder(x)\n",
    "        reconstructed = self.decoder(latent)\n",
    "        return reconstructed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "feebb594",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 128)\n",
    "        self.fc2 = nn.Linear(128, 1)\n",
    "        self.activation = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = nn.functional.relu(self.fc1(x))\n",
    "        return self.activation(self.fc2(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5ab33d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for Reconstruction Loss\n",
    "def reconstruction_loss(reconstructed, original):\n",
    "    return nn.functional.mse_loss(reconstructed, original)\n",
    "\n",
    "# Setting up models and optimizers\n",
    "generator = Generator(feature_size=25, num_heads=5, num_encoder_layers=3, output_dim=25).to(device)\n",
    "discriminator = Discriminator(input_dim=25).to(device)\n",
    "\n",
    "g_optimizer = optim.Adam(generator.parameters(), lr=0.001)\n",
    "d_optimizer = optim.Adam(discriminator.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e4fea34",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "218a1361",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Function for Training\n",
    "def train_epoch(generator, discriminator, loader, g_optimizer, d_optimizer, device, track_errors=False):\n",
    "    generator.train()\n",
    "    discriminator.train()\n",
    "    reconstruction_errors = []\n",
    "\n",
    "    for masked_data, real_data in loader:\n",
    "        masked_data, real_data = masked_data.to(device), real_data.to(device)\n",
    "        \n",
    "        # Forward pass through generator\n",
    "        generated_data = generator(masked_data)\n",
    "\n",
    "        # Discriminator training\n",
    "        d_optimizer.zero_grad()\n",
    "        real_pred = discriminator(real_data)\n",
    "        fake_pred = discriminator(generated_data.detach())\n",
    "        d_loss = -(torch.log(real_pred) + torch.log(1 - fake_pred)).mean()\n",
    "        d_loss.backward()\n",
    "        d_optimizer.step()\n",
    "\n",
    "        # Generator training\n",
    "        g_optimizer.zero_grad()\n",
    "        fake_pred = discriminator(generated_data)\n",
    "        g_loss = -torch.log(fake_pred).mean()\n",
    "        rec_loss = nn.functional.mse_loss(generated_data, real_data)  # Reconstruction loss\n",
    "        total_g_loss = g_loss + rec_loss\n",
    "        total_g_loss.backward()\n",
    "        g_optimizer.step()\n",
    "\n",
    "        # Optionally track reconstruction errors\n",
    "        if track_errors:\n",
    "            with torch.no_grad():\n",
    "                batch_errors = nn.functional.mse_loss(generated_data, real_data, reduction='none')\n",
    "                batch_errors = batch_errors.mean(dim=1)  # Mean error per sample\n",
    "                reconstruction_errors.extend(batch_errors.cpu().numpy())\n",
    "\n",
    "    return reconstruction_errors if track_errors else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ee054664",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1 completed.\n",
      "Anomaly Detection Threshold: 0.5\n"
     ]
    }
   ],
   "source": [
    "# Training the model and tracking errors\n",
    "num_epochs = 1\n",
    "error_tracking = []\n",
    "for epoch in range(num_epochs):\n",
    "    errors = train_epoch(generator, discriminator, train_loader, g_optimizer, d_optimizer, device, track_errors=True)\n",
    "    print(f'Epoch {epoch+1}/{num_epochs} completed.')\n",
    "\n",
    "# Determining threshold for anomaly detection\n",
    "threshold = 0.5\n",
    "print(f\"Anomaly Detection Threshold: {threshold}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba1fa14",
   "metadata": {},
   "source": [
    "#### Anomly Detection After Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "961749db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of anomalies detected: 0\n"
     ]
    }
   ],
   "source": [
    "#Function to detect anomolied\n",
    "def detect_anomalies(data_loader, generator, threshold, device):\n",
    "    anomalies = []\n",
    "    generator.eval()\n",
    "    with torch.no_grad():\n",
    "        for data, _ in data_loader:\n",
    "            data = data.to(device)\n",
    "            reconstructed_data = generator(data)\n",
    "            error = nn.functional.mse_loss(reconstructed_data, data, reduction='none')\n",
    "            error = error.mean(dim=1)\n",
    "            anomalies.extend((error > threshold).cpu().numpy())\n",
    "\n",
    "    return anomalies\n",
    "\n",
    "anomalies_detected = detect_anomalies(train_loader, generator, threshold, device)\n",
    "print(f\"Number of anomalies detected: {np.sum(anomalies_detected)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d0b99f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
